from fastapi import FastAPI, File, UploadFile, Form, Request
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

from ultralytics import YOLO
import whisper
import ffmpeg
import os
import tempfile
from datetime import datetime
import logging
from openai import OpenAI

# ‚úÖ Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ‚úÖ Initialize FastAPI
app = FastAPI()

# ‚úÖ Mount static and templates
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="public")

# ‚úÖ Serve frontend
@app.get("/", response_class=HTMLResponse)
def serve_frontend(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ‚úÖ Health check
@app.get("/api")
def read_root():
    return {"message": "PeaceWatch AI Backend is running"}

# ‚úÖ Load models
vision_model = YOLO("yolov8n.pt")
speech_model = whisper.load_model("base")

# ‚úÖ Setup OpenAI client
openai_key = os.getenv("OPENAI_API_KEY")
if not openai_key:
    raise RuntimeError("‚ùå OPENAI_API_KEY is not set in the environment.")
client = OpenAI(api_key=openai_key)

# ‚úÖ Analyze endpoint
@app.post("/analyze")
async def analyze(file: UploadFile = File(...), description: str = Form(...)):
    suffix = os.path.splitext(file.filename)[1]
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp:
        temp.write(await file.read())
        temp_path = temp.name

    result_summary = {
        "description": description,
        "timestamp": datetime.utcnow().isoformat()
    }

    try:
        # üß† MEDIA ANALYSIS
        if suffix.lower() in [".jpg", ".jpeg", ".png"]:
            result = vision_model(temp_path)[0]
            labels = result.names
            detected = []
            if result.boxes and result.boxes.cls is not None and len(result.boxes.cls) > 0:
                detected = [labels[int(cls)] for cls in result.boxes.cls]
            result_summary["image_detections"] = detected
            logger.info(f"Image detections: {detected}")

        elif suffix.lower() in [".mp4", ".mov", ".avi"]:
            frame_path = temp_path + "_frame.jpg"
            (
                ffmpeg
                .input(temp_path, ss=1)
                .filter('scale', 640, -1)
                .output(frame_path, vframes=1, **{'update': 1})
                .run(overwrite_output=True)
            )
            result = vision_model(frame_path)[0]
            labels = result.names
            detected = []
            if result.boxes and result.boxes.cls is not None and len(result.boxes.cls) > 0:
                detected = [labels[int(cls)] for cls in result.boxes.cls]
            result_summary["video_detections"] = detected
            logger.info(f"Video detections: {detected}")

        elif suffix.lower() in [".mp3", ".wav", ".m4a"]:
            transcription = speech_model.transcribe(temp_path)
            result_summary["audio_transcription"] = transcription['text']
            logger.info(f"Transcription: {transcription['text']}")

        # üß† GPT INCIDENT SUMMARY
        summary_prompt = f"""
        You are an AI assistant summarizing conflict reports. Here's the report:
        Description: {result_summary.get('description', '')}
        Detections: {result_summary.get('image_detections') or result_summary.get('video_detections')}
        Transcription: {result_summary.get('audio_transcription', '')}

        Summarize the incident in 2-3 lines for a human rights officer.
        """
        try:
            summary_response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that summarizes community safety incidents."},
                    {"role": "user", "content": summary_prompt}
                ]
            )
            summary = summary_response.choices[0].message.content.strip()
            result_summary["gpt_summary"] = summary
            logger.info("‚úÖ GPT summary generated.")
        except Exception as gpt_err:
            logger.warning(f"GPT summary error: {gpt_err}")
            result_summary["gpt_summary"] = f"‚ö†Ô∏è GPT summarization failed: {str(gpt_err)}"

        # üß≠ GPT USER GUIDANCE
        guidance_prompt = f"""
        Based on the following report, offer guidance for a civilian:
        - Description: {result_summary.get('description', '')}
        - Detections: {result_summary.get('image_detections') or result_summary.get('video_detections')}
        - Transcription: {result_summary.get('audio_transcription', '')}
        - Summary: {result_summary.get('gpt_summary', '')}

        Help the user understand:
        1. What this summary means.
        2. Why legal terms like "allegedly" are used.
        3. What more evidence (photo, timestamp, location) could help.
        4. Whether they should report this to authorities or not.

        Return helpful, plain-language advice.
        """
        try:
            guidance_response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You help people understand safety reports and give next-step advice."},
                    {"role": "user", "content": guidance_prompt}
                ]
            )
            guidance = guidance_response.choices[0].message.content.strip()
            result_summary["user_guidance"] = guidance
            logger.info("‚úÖ User guidance generated.")
        except Exception as gpt_guidance_err:
            logger.warning(f"GPT guidance error: {gpt_guidance_err}")
            result_summary["user_guidance"] = f"‚ö†Ô∏è Guidance generation failed: {str(gpt_guidance_err)}"

    except Exception as e:
        logger.error(f"Error processing file: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)
    finally:
        os.remove(temp_path)
        frame_path = temp_path + "_frame.jpg"
        if os.path.exists(frame_path):
            os.remove(frame_path)

    return result_summary
